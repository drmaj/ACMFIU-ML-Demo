{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Customer IDs\n",
    "## Demo for ACM FIU Workshop on Machine Learning - 3/20/2019\n",
    "\n",
    "Welcome!\n",
    "\n",
    "What follows is a quick demo of how to build a simple neural network classifier using pytorch to identify a customer id from log data.\n",
    "\n",
    "### Dataset Description\n",
    "\n",
    "This data comes from access logs for a popular piece of communication software. It logs various fields, including access times, IP adresses, operatings system used to log in, etc. This is from a real system so it has been anonymized to preserve both the company, and the user's privacy.\n",
    "\n",
    "* Anonymization process preserved the data structure: e.g. same result for tenant ids, user emails etc\n",
    "* Only see successful logins\n",
    "* Each person only associated with a single tenant e.g. CustomerID/username for example\n",
    "    * BrokeringUpn: abe.lincoln@usa.gov \n",
    "    * BrokeringUserFullName: abe.lincoln \n",
    "    * BrokeringUserName: USA\\abe.lincoln\n",
    "\n",
    "The full dataset is 1.9GB, so if you are interested in playing around with it, you can download it here: https://www.dropbox.com/s/sgotmtrrhtmk6b6/XAXD_events_all_time.anon.csv?dl=0. (Place it in the data folder if you want to re-run the preprocessing script.)\n",
    "\n",
    "Fundamentally, this is a dataset of logon events, where in each case we have:\n",
    "* Several fields that identify the user, with different levels of precision/trust/etc\n",
    "* Several fields that identify their associated organization, probably with customerId as the canonical \"tenant\" identifier, but there is also org-level information in the domain portion of the email address fields (e.g. BrokeringUpn) and in the second part of the BrokeringUserName field (i.e., after the \"\\\\\").\n",
    "* Fields containing the user's device name (ClientName), local OS (ClientPlatform) and version (ClientVersion), and IP address (I'm not sure the difference between the IP fields, and the original IPs were a mix of public/private IPs, but there's some information there)\n",
    "* A handful of various datetimes for specific sub-events associated with the logon\n",
    "* A few session IDs/keys. SIDs have some sub-structure insofar as stanzas may be common across records. E.g. one record might have value S-1-5-21-A-B-C-D, and another have value S-1-5-21-A-B-C-E.\n",
    "* A few other misc attributes like IsApp, etc\n",
    "\n",
    "Ideas  to look at:\n",
    "* How many users will log in tomorrow, next week, etc? Overall, by org, etc.\n",
    "* Are there different recognizable types of users based on behavior, usage patterns, etc?\n",
    "* Are there different recognizable types of orgs based on collective behaviors and characteristics of their users, etc?\n",
    "* Are there users who appear anomalous, either in general or suddenly one day, etc?\n",
    "* How confidently can a user's identity be predicted based on features like\n",
    "* Are there any discernable patterns around client platform, IP address, etc?\n",
    "* Any patterns, predictability, etc around time durations within events, e.g. difference between LaunchRequestTime and ValidateSessionTime\n",
    "* Are anonymous users (IsAnonymous==\"true\") different from identified users?\n",
    "* Maybe something about understanding why some fields have unexpected values. E.g. records where BrokeringUpn is missing, or has something other than an email address (i.e., no \"@\")\n",
    "\n",
    "### Now on to building our classifier!\n",
    "\n",
    "## Step 1 - imports\n",
    "\n",
    "We'll start by importing the necessary libraries and packages. Note: if you're running this notebook outside of colab, then you'll need to make sure the appropriate packages are installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch import FloatTensor, LongTensor, max, device, cuda\n",
    "from datasets import CustomerData\n",
    "from models import IdNet\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Let's do some Exploratory Data Analysis\n",
    "\n",
    "As I mentioned above, this data set is very large, so in an effort to speed things up, I've already preprocessed the data to remove and create a dataset with the features and labels, since we're going to approach this problem of creating a classifier to predict the customer id from the input features as a supervised learning problem.\n",
    "\n",
    "### Explore the raw data\n",
    "\n",
    "Let's take a look at the first few rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerId</th>\n",
       "      <th>st_weekday</th>\n",
       "      <th>st_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.395907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.176530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.563791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.660667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.904142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.525018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.083971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.762844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.584590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    customerId  st_weekday  st_seconds\n",
       "0            2    0.333333    0.395907\n",
       "1            3    0.000000    0.103960\n",
       "2            0    0.833333    0.176530\n",
       "3            1    0.500000    0.563791\n",
       "4            1    0.833333    0.660667\n",
       "8            3    1.000000    0.904142\n",
       "10           2    0.000000    0.525018\n",
       "13           3    0.666667    0.083971\n",
       "16           0    0.000000    0.762844\n",
       "17           0    0.000000    0.584590"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's load the data and take a look\n",
    "train_df = pd.read_pickle(\"data/train_data.pickle\")\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the table, that we have two features, *st_weekday* and *st_seconds*, and one label, *customerId*. The features have already been normalized from 0 to 1 (it is easier for the net to use these as inputs and scaling gives equal weight the features so that no one feature dominates). The weekday column represents the day of the week, with 0 corresponding to Monday, and 1 corresponding to Sunday. The seconds column represents the number of seconds since 12:00AM. And finally, the customer id has been encoded from the true customer id to an integer from 0 to 3. There are way more than 4 customer ids in the raw dataset, however, the top four customers by volume of transaction records were selected and then re-balanced so that each customer id is represented an equal number of times.\n",
    "\n",
    "Let's go ahead and do a scatter plot with the data to see if we can detect some usage patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cmap = cm.get_cmap('Spectral')\n",
    "train_df.sample(frac=0.05).plot.scatter(x='st_weekday',\n",
    "                                        y='st_seconds',\n",
    "                                        c='customerId',\n",
    "                                        cmap=cmap,\n",
    "                                        edgecolor=None,\n",
    "                                        alpha=0.5,\n",
    "                                        # s=100,\n",
    "                                        marker=\"_\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like some customers log in on different days, as well as at different points in the day. We may be able to build an ML algorithm to detect these patterns and predict, based on time of day and day of the week, which customers are loggin in.\n",
    "\n",
    "## Step 3 - Load the data for training and testing\n",
    "\n",
    "Here, there are a few classes that we need. One, *CustomerData*, is a custom dataset class that is derived from PyTorch's base dataset class. You can check out the implementation in the *datasets.py* file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the data for training the model\n",
    "train_dataset = CustomerData('data/train_data.pickle')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Setup the data for testing the model\n",
    "test_dataset = CustomerData('data/test_data.pickle')\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Setup our hyperparameters for training\n",
    "\n",
    "**Hyperparameters** are parameters whose value are set before the learning process begins. As opposed to **parameters**, which refer to the parameters of the Machine Learning model that are learned during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "num_epochs = 1\n",
    "num_labels = 4\n",
    "num_features = 2\n",
    "log_interval = 100\n",
    "n_hidden = 128\n",
    "ngpu = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Instantiate the Model, Optimizer, and Loss functions\n",
    "\n",
    "Here, we'll instantiate the class that embodies the Neural Network. In our case, it as multilayer neural network with an input layer, an output layer, and 3 hidden layers. Each layer is successively smaller by a factor of two. The output of each layer is sent through a Rectified Linear Unit (ReLU) activation function, which adds a non-linearity to the network that allows it to learn non-linear mappings. We'll also check to see if we will be training the model on a GPU or on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = device(\"cuda:0\" if (cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "classifier = IdNet(num_features, n_hidden, num_labels)\n",
    "classifier.to(device)\n",
    "optimizer = SGD(classifier.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Training and testing!\n",
    "\n",
    "Now we get to the good stuff! We'll start iteratively training our network, based on the loss function and optimizer that we selected above. Note: there are many loss functions and optimizers to choose from. Which one you select largerly depends on many factors, such as the structure of your network, the type of data that you are using, and the problem you are trying to solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar 15 11:29:22 2019\t Epoch 1:\t[800/123277]\tloss: 1.367563, acc: 0.253\n",
      "Fri Mar 15 11:29:23 2019\t Epoch 1:\t[1600/123277]\tloss: 1.397730, acc: 0.253\n",
      "Fri Mar 15 11:29:23 2019\t Epoch 1:\t[2400/123277]\tloss: 1.399872, acc: 0.242\n",
      "Fri Mar 15 11:29:24 2019\t Epoch 1:\t[3200/123277]\tloss: 1.374699, acc: 0.243\n",
      "Fri Mar 15 11:29:25 2019\t Epoch 1:\t[4000/123277]\tloss: 1.362087, acc: 0.248\n",
      "Fri Mar 15 11:29:26 2019\t Epoch 1:\t[4800/123277]\tloss: 1.374463, acc: 0.246\n",
      "Fri Mar 15 11:29:27 2019\t Epoch 1:\t[5600/123277]\tloss: 1.357518, acc: 0.251\n",
      "Fri Mar 15 11:29:27 2019\t Epoch 1:\t[6400/123277]\tloss: 1.382271, acc: 0.259\n",
      "Fri Mar 15 11:29:28 2019\t Epoch 1:\t[7200/123277]\tloss: 1.412704, acc: 0.268\n",
      "Fri Mar 15 11:29:29 2019\t Epoch 1:\t[8000/123277]\tloss: 1.546174, acc: 0.276\n",
      "Fri Mar 15 11:29:30 2019\t Epoch 1:\t[8800/123277]\tloss: 1.036819, acc: 0.287\n",
      "Fri Mar 15 11:29:30 2019\t Epoch 1:\t[9600/123277]\tloss: 1.335943, acc: 0.301\n",
      "Fri Mar 15 11:29:31 2019\t Epoch 1:\t[10400/123277]\tloss: 0.817843, acc: 0.315\n",
      "Fri Mar 15 11:29:32 2019\t Epoch 1:\t[11200/123277]\tloss: 1.003201, acc: 0.328\n",
      "Fri Mar 15 11:29:33 2019\t Epoch 1:\t[12000/123277]\tloss: 1.145228, acc: 0.343\n",
      "Fri Mar 15 11:29:33 2019\t Epoch 1:\t[12800/123277]\tloss: 0.965442, acc: 0.354\n",
      "Fri Mar 15 11:29:34 2019\t Epoch 1:\t[13600/123277]\tloss: 1.127120, acc: 0.365\n",
      "Fri Mar 15 11:29:35 2019\t Epoch 1:\t[14400/123277]\tloss: 1.832307, acc: 0.375\n",
      "Fri Mar 15 11:29:36 2019\t Epoch 1:\t[15200/123277]\tloss: 0.898865, acc: 0.386\n",
      "Fri Mar 15 11:29:36 2019\t Epoch 1:\t[16000/123277]\tloss: 0.795914, acc: 0.395\n",
      "Fri Mar 15 11:29:37 2019\t Epoch 1:\t[16800/123277]\tloss: 0.861494, acc: 0.404\n",
      "Fri Mar 15 11:29:38 2019\t Epoch 1:\t[17600/123277]\tloss: 0.847656, acc: 0.412\n",
      "Fri Mar 15 11:29:39 2019\t Epoch 1:\t[18400/123277]\tloss: 1.046613, acc: 0.418\n",
      "Fri Mar 15 11:29:39 2019\t Epoch 1:\t[19200/123277]\tloss: 0.945320, acc: 0.425\n",
      "Fri Mar 15 11:29:40 2019\t Epoch 1:\t[20000/123277]\tloss: 0.640822, acc: 0.432\n",
      "Fri Mar 15 11:29:41 2019\t Epoch 1:\t[20800/123277]\tloss: 0.796728, acc: 0.438\n",
      "Fri Mar 15 11:29:42 2019\t Epoch 1:\t[21600/123277]\tloss: 1.186160, acc: 0.444\n",
      "Fri Mar 15 11:29:43 2019\t Epoch 1:\t[22400/123277]\tloss: 1.159699, acc: 0.449\n",
      "Fri Mar 15 11:29:43 2019\t Epoch 1:\t[23200/123277]\tloss: 1.039412, acc: 0.453\n",
      "Fri Mar 15 11:29:44 2019\t Epoch 1:\t[24000/123277]\tloss: 0.890225, acc: 0.458\n",
      "Fri Mar 15 11:29:45 2019\t Epoch 1:\t[24800/123277]\tloss: 1.385068, acc: 0.462\n",
      "Fri Mar 15 11:29:46 2019\t Epoch 1:\t[25600/123277]\tloss: 0.898393, acc: 0.466\n",
      "Fri Mar 15 11:29:47 2019\t Epoch 1:\t[26400/123277]\tloss: 1.022371, acc: 0.471\n",
      "Fri Mar 15 11:29:47 2019\t Epoch 1:\t[27200/123277]\tloss: 0.856410, acc: 0.474\n",
      "Fri Mar 15 11:29:48 2019\t Epoch 1:\t[28000/123277]\tloss: 1.530207, acc: 0.479\n",
      "Fri Mar 15 11:29:49 2019\t Epoch 1:\t[28800/123277]\tloss: 1.003981, acc: 0.483\n",
      "Fri Mar 15 11:29:50 2019\t Epoch 1:\t[29600/123277]\tloss: 0.704534, acc: 0.486\n",
      "Fri Mar 15 11:29:50 2019\t Epoch 1:\t[30400/123277]\tloss: 0.738589, acc: 0.489\n",
      "Fri Mar 15 11:29:51 2019\t Epoch 1:\t[31200/123277]\tloss: 0.907200, acc: 0.491\n",
      "Fri Mar 15 11:29:52 2019\t Epoch 1:\t[32000/123277]\tloss: 1.037131, acc: 0.494\n",
      "Fri Mar 15 11:29:53 2019\t Epoch 1:\t[32800/123277]\tloss: 0.590371, acc: 0.496\n",
      "Fri Mar 15 11:29:53 2019\t Epoch 1:\t[33600/123277]\tloss: 0.639948, acc: 0.499\n",
      "Fri Mar 15 11:29:54 2019\t Epoch 1:\t[34400/123277]\tloss: 0.996044, acc: 0.502\n",
      "Fri Mar 15 11:29:55 2019\t Epoch 1:\t[35200/123277]\tloss: 0.806900, acc: 0.504\n",
      "Fri Mar 15 11:29:56 2019\t Epoch 1:\t[36000/123277]\tloss: 1.079169, acc: 0.507\n",
      "Fri Mar 15 11:29:56 2019\t Epoch 1:\t[36800/123277]\tloss: 1.040501, acc: 0.509\n",
      "Fri Mar 15 11:29:57 2019\t Epoch 1:\t[37600/123277]\tloss: 0.926350, acc: 0.511\n",
      "Fri Mar 15 11:29:58 2019\t Epoch 1:\t[38400/123277]\tloss: 1.184725, acc: 0.514\n",
      "Fri Mar 15 11:29:59 2019\t Epoch 1:\t[39200/123277]\tloss: 1.257544, acc: 0.516\n",
      "Fri Mar 15 11:30:00 2019\t Epoch 1:\t[40000/123277]\tloss: 0.582171, acc: 0.518\n",
      "Fri Mar 15 11:30:00 2019\t Epoch 1:\t[40800/123277]\tloss: 1.763712, acc: 0.519\n",
      "Fri Mar 15 11:30:01 2019\t Epoch 1:\t[41600/123277]\tloss: 1.027278, acc: 0.521\n",
      "Fri Mar 15 11:30:02 2019\t Epoch 1:\t[42400/123277]\tloss: 0.873519, acc: 0.523\n",
      "Fri Mar 15 11:30:03 2019\t Epoch 1:\t[43200/123277]\tloss: 0.931411, acc: 0.524\n",
      "Fri Mar 15 11:30:04 2019\t Epoch 1:\t[44000/123277]\tloss: 1.159464, acc: 0.526\n",
      "Fri Mar 15 11:30:04 2019\t Epoch 1:\t[44800/123277]\tloss: 1.086740, acc: 0.528\n",
      "Fri Mar 15 11:30:05 2019\t Epoch 1:\t[45600/123277]\tloss: 1.532452, acc: 0.530\n",
      "Fri Mar 15 11:30:06 2019\t Epoch 1:\t[46400/123277]\tloss: 1.054582, acc: 0.531\n",
      "Fri Mar 15 11:30:07 2019\t Epoch 1:\t[47200/123277]\tloss: 1.196968, acc: 0.533\n",
      "Fri Mar 15 11:30:07 2019\t Epoch 1:\t[48000/123277]\tloss: 1.094677, acc: 0.534\n",
      "Fri Mar 15 11:30:08 2019\t Epoch 1:\t[48800/123277]\tloss: 1.231568, acc: 0.536\n",
      "Fri Mar 15 11:30:09 2019\t Epoch 1:\t[49600/123277]\tloss: 0.734078, acc: 0.537\n",
      "Fri Mar 15 11:30:09 2019\t Epoch 1:\t[50400/123277]\tloss: 0.870556, acc: 0.539\n",
      "Fri Mar 15 11:30:10 2019\t Epoch 1:\t[51200/123277]\tloss: 1.501676, acc: 0.540\n",
      "Fri Mar 15 11:30:11 2019\t Epoch 1:\t[52000/123277]\tloss: 0.771243, acc: 0.541\n",
      "Fri Mar 15 11:30:12 2019\t Epoch 1:\t[52800/123277]\tloss: 1.195836, acc: 0.542\n",
      "Fri Mar 15 11:30:12 2019\t Epoch 1:\t[53600/123277]\tloss: 1.437306, acc: 0.543\n",
      "Fri Mar 15 11:30:13 2019\t Epoch 1:\t[54400/123277]\tloss: 0.848388, acc: 0.545\n",
      "Fri Mar 15 11:30:14 2019\t Epoch 1:\t[55200/123277]\tloss: 0.813429, acc: 0.545\n",
      "Fri Mar 15 11:30:15 2019\t Epoch 1:\t[56000/123277]\tloss: 0.387645, acc: 0.547\n",
      "Fri Mar 15 11:30:15 2019\t Epoch 1:\t[56800/123277]\tloss: 0.775566, acc: 0.548\n",
      "Fri Mar 15 11:30:16 2019\t Epoch 1:\t[57600/123277]\tloss: 0.923374, acc: 0.549\n",
      "Fri Mar 15 11:30:17 2019\t Epoch 1:\t[58400/123277]\tloss: 1.002102, acc: 0.550\n",
      "Fri Mar 15 11:30:18 2019\t Epoch 1:\t[59200/123277]\tloss: 0.661470, acc: 0.551\n",
      "Fri Mar 15 11:30:19 2019\t Epoch 1:\t[60000/123277]\tloss: 0.701578, acc: 0.552\n",
      "Fri Mar 15 11:30:20 2019\t Epoch 1:\t[60800/123277]\tloss: 0.672197, acc: 0.553\n",
      "Fri Mar 15 11:30:21 2019\t Epoch 1:\t[61600/123277]\tloss: 0.945734, acc: 0.554\n",
      "Fri Mar 15 11:30:22 2019\t Epoch 1:\t[62400/123277]\tloss: 1.115553, acc: 0.555\n",
      "Fri Mar 15 11:30:22 2019\t Epoch 1:\t[63200/123277]\tloss: 1.000879, acc: 0.556\n",
      "Fri Mar 15 11:30:23 2019\t Epoch 1:\t[64000/123277]\tloss: 0.931906, acc: 0.557\n",
      "Fri Mar 15 11:30:24 2019\t Epoch 1:\t[64800/123277]\tloss: 1.044853, acc: 0.557\n",
      "Fri Mar 15 11:30:25 2019\t Epoch 1:\t[65600/123277]\tloss: 0.765055, acc: 0.558\n",
      "Fri Mar 15 11:30:26 2019\t Epoch 1:\t[66400/123277]\tloss: 1.325640, acc: 0.559\n",
      "Fri Mar 15 11:30:27 2019\t Epoch 1:\t[67200/123277]\tloss: 0.685983, acc: 0.561\n",
      "Fri Mar 15 11:30:28 2019\t Epoch 1:\t[68000/123277]\tloss: 0.685890, acc: 0.561\n",
      "Fri Mar 15 11:30:29 2019\t Epoch 1:\t[68800/123277]\tloss: 0.596827, acc: 0.562\n",
      "Fri Mar 15 11:30:29 2019\t Epoch 1:\t[69600/123277]\tloss: 1.026389, acc: 0.562\n",
      "Fri Mar 15 11:30:30 2019\t Epoch 1:\t[70400/123277]\tloss: 0.639598, acc: 0.563\n",
      "Fri Mar 15 11:30:31 2019\t Epoch 1:\t[71200/123277]\tloss: 0.754099, acc: 0.564\n",
      "Fri Mar 15 11:30:32 2019\t Epoch 1:\t[72000/123277]\tloss: 0.794612, acc: 0.565\n",
      "Fri Mar 15 11:30:32 2019\t Epoch 1:\t[72800/123277]\tloss: 1.017816, acc: 0.566\n",
      "Fri Mar 15 11:30:33 2019\t Epoch 1:\t[73600/123277]\tloss: 0.549159, acc: 0.566\n",
      "Fri Mar 15 11:30:34 2019\t Epoch 1:\t[74400/123277]\tloss: 1.386096, acc: 0.567\n",
      "Fri Mar 15 11:30:35 2019\t Epoch 1:\t[75200/123277]\tloss: 1.580647, acc: 0.568\n",
      "Fri Mar 15 11:30:35 2019\t Epoch 1:\t[76000/123277]\tloss: 0.758485, acc: 0.568\n",
      "Fri Mar 15 11:30:36 2019\t Epoch 1:\t[76800/123277]\tloss: 0.698832, acc: 0.569\n",
      "Fri Mar 15 11:30:37 2019\t Epoch 1:\t[77600/123277]\tloss: 1.528794, acc: 0.569\n",
      "Fri Mar 15 11:30:38 2019\t Epoch 1:\t[78400/123277]\tloss: 0.974669, acc: 0.570\n",
      "Fri Mar 15 11:30:38 2019\t Epoch 1:\t[79200/123277]\tloss: 0.837705, acc: 0.571\n",
      "Fri Mar 15 11:30:39 2019\t Epoch 1:\t[80000/123277]\tloss: 0.737533, acc: 0.571\n",
      "Fri Mar 15 11:30:40 2019\t Epoch 1:\t[80800/123277]\tloss: 0.607147, acc: 0.572\n",
      "Fri Mar 15 11:30:41 2019\t Epoch 1:\t[81600/123277]\tloss: 0.859845, acc: 0.572\n",
      "Fri Mar 15 11:30:41 2019\t Epoch 1:\t[82400/123277]\tloss: 1.249597, acc: 0.572\n",
      "Fri Mar 15 11:30:42 2019\t Epoch 1:\t[83200/123277]\tloss: 0.584178, acc: 0.572\n",
      "Fri Mar 15 11:30:43 2019\t Epoch 1:\t[84000/123277]\tloss: 0.804367, acc: 0.573\n",
      "Fri Mar 15 11:30:44 2019\t Epoch 1:\t[84800/123277]\tloss: 0.785176, acc: 0.573\n",
      "Fri Mar 15 11:30:44 2019\t Epoch 1:\t[85600/123277]\tloss: 1.040629, acc: 0.573\n",
      "Fri Mar 15 11:30:45 2019\t Epoch 1:\t[86400/123277]\tloss: 0.428252, acc: 0.574\n",
      "Fri Mar 15 11:30:46 2019\t Epoch 1:\t[87200/123277]\tloss: 0.891180, acc: 0.574\n",
      "Fri Mar 15 11:30:46 2019\t Epoch 1:\t[88000/123277]\tloss: 1.037536, acc: 0.575\n",
      "Fri Mar 15 11:30:47 2019\t Epoch 1:\t[88800/123277]\tloss: 1.203417, acc: 0.575\n",
      "Fri Mar 15 11:30:48 2019\t Epoch 1:\t[89600/123277]\tloss: 0.867719, acc: 0.575\n",
      "Fri Mar 15 11:30:49 2019\t Epoch 1:\t[90400/123277]\tloss: 0.969455, acc: 0.576\n",
      "Fri Mar 15 11:30:50 2019\t Epoch 1:\t[91200/123277]\tloss: 0.865012, acc: 0.576\n",
      "Fri Mar 15 11:30:50 2019\t Epoch 1:\t[92000/123277]\tloss: 1.011275, acc: 0.577\n",
      "Fri Mar 15 11:30:51 2019\t Epoch 1:\t[92800/123277]\tloss: 0.897364, acc: 0.577\n",
      "Fri Mar 15 11:30:52 2019\t Epoch 1:\t[93600/123277]\tloss: 0.765104, acc: 0.577\n",
      "Fri Mar 15 11:30:53 2019\t Epoch 1:\t[94400/123277]\tloss: 1.046606, acc: 0.577\n",
      "Fri Mar 15 11:30:53 2019\t Epoch 1:\t[95200/123277]\tloss: 0.546476, acc: 0.578\n",
      "Fri Mar 15 11:30:54 2019\t Epoch 1:\t[96000/123277]\tloss: 0.991754, acc: 0.578\n",
      "Fri Mar 15 11:30:55 2019\t Epoch 1:\t[96800/123277]\tloss: 0.829503, acc: 0.578\n",
      "Fri Mar 15 11:30:56 2019\t Epoch 1:\t[97600/123277]\tloss: 0.979655, acc: 0.579\n",
      "Fri Mar 15 11:30:56 2019\t Epoch 1:\t[98400/123277]\tloss: 0.935666, acc: 0.579\n",
      "Fri Mar 15 11:30:57 2019\t Epoch 1:\t[99200/123277]\tloss: 1.065450, acc: 0.579\n",
      "Fri Mar 15 11:30:58 2019\t Epoch 1:\t[100000/123277]\tloss: 1.086109, acc: 0.579\n",
      "Fri Mar 15 11:30:59 2019\t Epoch 1:\t[100800/123277]\tloss: 1.004090, acc: 0.579\n",
      "Fri Mar 15 11:30:59 2019\t Epoch 1:\t[101600/123277]\tloss: 0.568932, acc: 0.580\n",
      "Fri Mar 15 11:31:00 2019\t Epoch 1:\t[102400/123277]\tloss: 0.867111, acc: 0.580\n",
      "Fri Mar 15 11:31:01 2019\t Epoch 1:\t[103200/123277]\tloss: 0.654415, acc: 0.581\n",
      "Fri Mar 15 11:31:02 2019\t Epoch 1:\t[104000/123277]\tloss: 1.044017, acc: 0.581\n",
      "Fri Mar 15 11:31:02 2019\t Epoch 1:\t[104800/123277]\tloss: 0.925060, acc: 0.581\n",
      "Fri Mar 15 11:31:03 2019\t Epoch 1:\t[105600/123277]\tloss: 1.031911, acc: 0.582\n",
      "Fri Mar 15 11:31:04 2019\t Epoch 1:\t[106400/123277]\tloss: 0.708372, acc: 0.582\n",
      "Fri Mar 15 11:31:05 2019\t Epoch 1:\t[107200/123277]\tloss: 0.995465, acc: 0.583\n",
      "Fri Mar 15 11:31:05 2019\t Epoch 1:\t[108000/123277]\tloss: 1.008874, acc: 0.583\n",
      "Fri Mar 15 11:31:06 2019\t Epoch 1:\t[108800/123277]\tloss: 1.061161, acc: 0.583\n",
      "Fri Mar 15 11:31:07 2019\t Epoch 1:\t[109600/123277]\tloss: 0.695931, acc: 0.583\n",
      "Fri Mar 15 11:31:08 2019\t Epoch 1:\t[110400/123277]\tloss: 0.883684, acc: 0.583\n",
      "Fri Mar 15 11:31:08 2019\t Epoch 1:\t[111200/123277]\tloss: 0.689384, acc: 0.584\n",
      "Fri Mar 15 11:31:09 2019\t Epoch 1:\t[112000/123277]\tloss: 0.870406, acc: 0.584\n",
      "Fri Mar 15 11:31:10 2019\t Epoch 1:\t[112800/123277]\tloss: 1.242507, acc: 0.584\n",
      "Fri Mar 15 11:31:11 2019\t Epoch 1:\t[113600/123277]\tloss: 0.973459, acc: 0.584\n",
      "Fri Mar 15 11:31:11 2019\t Epoch 1:\t[114400/123277]\tloss: 0.915503, acc: 0.584\n",
      "Fri Mar 15 11:31:12 2019\t Epoch 1:\t[115200/123277]\tloss: 1.225953, acc: 0.584\n",
      "Fri Mar 15 11:31:13 2019\t Epoch 1:\t[116000/123277]\tloss: 1.308884, acc: 0.585\n",
      "Fri Mar 15 11:31:14 2019\t Epoch 1:\t[116800/123277]\tloss: 1.207166, acc: 0.585\n",
      "Fri Mar 15 11:31:14 2019\t Epoch 1:\t[117600/123277]\tloss: 0.566652, acc: 0.585\n",
      "Fri Mar 15 11:31:15 2019\t Epoch 1:\t[118400/123277]\tloss: 0.560648, acc: 0.585\n",
      "Fri Mar 15 11:31:16 2019\t Epoch 1:\t[119200/123277]\tloss: 0.650742, acc: 0.586\n",
      "Fri Mar 15 11:31:17 2019\t Epoch 1:\t[120000/123277]\tloss: 0.810464, acc: 0.586\n",
      "Fri Mar 15 11:31:17 2019\t Epoch 1:\t[120800/123277]\tloss: 1.180421, acc: 0.586\n",
      "Fri Mar 15 11:31:18 2019\t Epoch 1:\t[121600/123277]\tloss: 1.268939, acc: 0.586\n",
      "Fri Mar 15 11:31:19 2019\t Epoch 1:\t[122400/123277]\tloss: 0.756195, acc: 0.587\n",
      "Fri Mar 15 11:31:20 2019\t Epoch 1:\t[123200/123277]\tloss: 0.920136, acc: 0.587\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    count = 0\n",
    "    correct_cnt = 0\n",
    "    total_cnt = 0\n",
    "    for batch_id, (data, labels) in enumerate(train_dataloader):\n",
    "        n_batch = len(labels)\n",
    "        optimizer.zero_grad()\n",
    "        data, labels = data.type(FloatTensor).to(device), labels.type(LongTensor).to(device)\n",
    "        out = classifier(data)\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, pred_label = max(out.data, 1)\n",
    "        total_cnt += data.data.size()[0]\n",
    "        correct_cnt += (pred_label == labels.data).sum()\n",
    "        count += n_batch\n",
    "        # losses.append(loss.data.mean())\n",
    "        if (batch_id + 1) % log_interval == 0:\n",
    "            accuracy = correct_cnt.data.cpu().numpy() * 1.0 / total_cnt\n",
    "            # losses_mean = np.mean(losses)\n",
    "            message = \"{}\\t Epoch {}:\\t[{}/{}]\\tloss: {:.6f}, acc: {:.3f}\".format(time.ctime(), epoch + 1, count,\n",
    "                                                                                  len(train_dataset), loss.item(),\n",
    "                                                                                  accuracy)\n",
    "            print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad!! Our model trained on the data and achieved an accuracy of almosts 59%. It's definitely doing better than just a random guess (which would be about 25% accuracy on average for four customer ids). But let's see how it does on the test dataset, which the model has never seen.\n",
    "\n",
    "## Step 7 - Validate the results\n",
    "\n",
    "After we're done training, we want to see how good our model really is. For that, we'll basically repeat the above inner loop, using the testing dataset instead and seeing how good our model is doing as we process the data. We'll also have to tell PyTorch not to update the model parameters, since we've already done that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> Epoch: 1, batch index: 100, test loss: 0.916242, acc: 0.606\n",
      "==>>> Epoch: 1, batch index: 200, test loss: 1.013640, acc: 0.610\n",
      "==>>> Epoch: 1, batch index: 300, test loss: 1.226806, acc: 0.619\n",
      "==>>> Epoch: 1, batch index: 400, test loss: 0.828746, acc: 0.627\n",
      "==>>> Epoch: 1, batch index: 500, test loss: 0.734014, acc: 0.625\n",
      "==>>> Epoch: 1, batch index: 600, test loss: 0.987053, acc: 0.629\n",
      "==>>> Epoch: 1, batch index: 700, test loss: 1.063580, acc: 0.628\n",
      "==>>> Epoch: 1, batch index: 800, test loss: 0.816766, acc: 0.625\n",
      "==>>> Epoch: 1, batch index: 900, test loss: 0.776856, acc: 0.624\n",
      "==>>> Epoch: 1, batch index: 1000, test loss: 0.745206, acc: 0.624\n",
      "==>>> Epoch: 1, batch index: 1100, test loss: 1.000431, acc: 0.621\n",
      "==>>> Epoch: 1, batch index: 1200, test loss: 0.899086, acc: 0.622\n",
      "==>>> Epoch: 1, batch index: 1300, test loss: 1.356267, acc: 0.621\n",
      "==>>> Epoch: 1, batch index: 1400, test loss: 1.203797, acc: 0.619\n",
      "==>>> Epoch: 1, batch index: 1500, test loss: 0.775486, acc: 0.618\n",
      "==>>> Epoch: 1, batch index: 1600, test loss: 1.083682, acc: 0.617\n",
      "==>>> Epoch: 1, batch index: 1700, test loss: 0.381231, acc: 0.618\n",
      "==>>> Epoch: 1, batch index: 1800, test loss: 0.887802, acc: 0.619\n",
      "==>>> Epoch: 1, batch index: 1900, test loss: 1.059779, acc: 0.619\n",
      "==>>> Epoch: 1, batch index: 2000, test loss: 1.580101, acc: 0.618\n",
      "==>>> Epoch: 1, batch index: 2100, test loss: 1.448920, acc: 0.619\n",
      "==>>> Epoch: 1, batch index: 2200, test loss: 0.423089, acc: 0.619\n",
      "==>>> Epoch: 1, batch index: 2300, test loss: 1.089022, acc: 0.619\n",
      "==>>> Epoch: 1, batch index: 2400, test loss: 0.992446, acc: 0.619\n",
      "==>>> Epoch: 1, batch index: 2500, test loss: 0.898918, acc: 0.619\n",
      "==>>> Epoch: 1, batch index: 2600, test loss: 0.519632, acc: 0.620\n",
      "==>>> Epoch: 1, batch index: 2700, test loss: 0.691944, acc: 0.619\n",
      "==>>> Epoch: 1, batch index: 2800, test loss: 1.069928, acc: 0.619\n",
      "==>>> Epoch: 1, batch index: 2900, test loss: 0.992966, acc: 0.618\n",
      "==>>> Epoch: 1, batch index: 3000, test loss: 0.644660, acc: 0.619\n",
      "==>>> Epoch: 1, batch index: 3100, test loss: 0.727963, acc: 0.619\n",
      "==>>> Epoch: 1, batch index: 3200, test loss: 0.876001, acc: 0.619\n",
      "==>>> Epoch: 1, batch index: 3300, test loss: 0.764727, acc: 0.618\n",
      "==>>> Epoch: 1, batch index: 3400, test loss: 1.063946, acc: 0.619\n",
      "==>>> Epoch: 1, batch index: 3500, test loss: 0.990463, acc: 0.619\n",
      "==>>> Epoch: 1, batch index: 3600, test loss: 1.202792, acc: 0.620\n",
      "==>>> Epoch: 1, batch index: 3700, test loss: 0.925228, acc: 0.619\n",
      "==>>> Epoch: 1, batch index: 3800, test loss: 0.863558, acc: 0.619\n",
      "==>>> Epoch: 1, batch index: 3900, test loss: 0.666376, acc: 0.619\n",
      "==>>> Epoch: 1, batch index: 4000, test loss: 1.379005, acc: 0.619\n",
      "==>>> Epoch: 1, batch index: 4100, test loss: 1.055380, acc: 0.619\n",
      "==>>> Epoch: 1, batch index: 4200, test loss: 0.728286, acc: 0.619\n",
      "==>>> Epoch: 1, batch index: 4300, test loss: 0.944296, acc: 0.620\n",
      "==>>> Epoch: 1, batch index: 4400, test loss: 0.601905, acc: 0.620\n",
      "==>>> Epoch: 1, batch index: 4425, test loss: 1.090900, acc: 0.620\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct_cnt = 0\n",
    "    total_cnt = 0\n",
    "    testing_iter_num = 0\n",
    "    for batch_id, (data, labels) in enumerate(test_dataloader):\n",
    "        testing_iter_num += 1\n",
    "        data, labels = data.type(FloatTensor).to(device), labels.type(LongTensor).to(device)\n",
    "        out = classifier(data)\n",
    "        loss = criterion(out, labels)\n",
    "        _, pred_label = torch.max(out.data, 1)\n",
    "        total_cnt += data.data.size()[0]\n",
    "        correct_cnt += (pred_label == labels.data).sum()\n",
    "        if (batch_id + 1) % 100 == 0 or (batch_id + 1) == len(test_dataloader):\n",
    "            accuracy = correct_cnt.data.cpu().numpy() * 1.0 / total_cnt\n",
    "            message = '==>>> Epoch: {}, batch index: {}, test loss: {:.6f}, acc: {:.3f}'.format(\n",
    "                    epoch + 1, batch_id + 1, loss.item(), accuracy)\n",
    "            print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 - Jump up and down!\n",
    "\n",
    "Wow! It seems like our network is doing even better that it did in training. We get to a whopping 62% accuracy!\n",
    "\n",
    "## Step 9 - Where to go from here?\n",
    "\n",
    "Although we are certainly doing better than a random guess at predicting customer ids, we still have a long way to go. Still, this is really impressive, since we only used two features. What this points to is that although we can find a mapping, we still need more information for our prediction accuracy to go up. In fact, if you continue to train the network for more epoch, you'll quickly see that the network accuracy tops off at around 63%. We'll have to go back through the raw data to see if there are other things that we can use as features, such as using the IP addresses or the device type. But be careful, this might actually make things worse. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
